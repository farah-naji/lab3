{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f3ec65-4cd8-4aea-a418-bc8fbc9df9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji==2.12.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d61b057-b85d-430f-92f2-b0348ea0bd25/lib/python3.12/site-packages (2.12.1)\nRequirement already satisfied: typing-extensions>=4.7.0 in /databricks/python3/lib/python3.12/site-packages (from emoji==2.12.1) (4.11.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers==4.57.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (4.57.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nLooking in indexes: https://download.pytorch.org/whl/cpu\nRequirement already satisfied: torch==2.2.2+cpu in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d61b057-b85d-430f-92f2-b0348ea0bd25/lib/python3.12/site-packages (2.2.2+cpu)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Minimal installs – do NOT touch numpy/pyarrow\n",
    "\n",
    "# emojis for cleaning\n",
    "%pip install emoji==2.12.1\n",
    "\n",
    "# transformers for BERT (no deps so we don't break numpy/pyarrow)\n",
    "%pip install \"transformers==4.57.1\" --no-deps\n",
    "\n",
    "# CPU-only torch (no CUDA, no libcusparseLt.so.0)\n",
    "%pip install \"torch==2.2.2+cpu\" \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu \\\n",
    "    --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8cf9fd-fbef-42ba-a952-bb25de18aa47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf0a8a8-89d9-4e6f-aa9e-44283e9113d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub==0.36.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub==0.36.0) (3.15.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (6.0.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (2.32.2)\nRequirement already satisfied: tqdm>=4.42.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (4.11.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from huggingface_hub==0.36.0) (1.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub==0.36.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub==0.36.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub==0.36.0) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub==0.36.0) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install \"huggingface_hub==0.36.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f913a7c3-3848-4f05-876d-2425ef07c459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.2.2+cpu\nHF Hub: 0.36.0\nModel loaded OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import huggingface_hub\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"HF Hub:\", huggingface_hub.__version__)\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model loaded OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7845b286-eee9-4138-833b-4d3e6f1f17bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key set: OK\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf, col, length, size, split\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, ArrayType, StructType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# use transformers + torch instead of sentence_transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Set storage key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.goodreadsreviews60104384.dfs.core.windows.net\",\n",
    "    \"6GsPhkGZNxjNX3ph9pR77Kb5jZVywf/ZnBgwaKQPBtToe+sBA9pAoHqA5w7Ls5atRqGPbG8CyhNu+ASts8Yhzw==\"\n",
    ")\n",
    "print(\"Key set: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf83e60-819a-494b-9d29-9b8c6c81779a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old deleted: OK\n"
     ]
    }
   ],
   "source": [
    "# Delete old\n",
    "dbutils.fs.rm(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/feature_v2/\", recurse=True)\n",
    "dbutils.fs.rm(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/features_v2/\", recurse=True)\n",
    "print(\"Old deleted: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40753fca-44b4-4319-b7c4-93d333132823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated rows: 15739524\n+--------------------+--------+--------------------+---------+---------------+--------------------+------+--------------------+--------+-------+----------+\n|           review_id| book_id|               title|author_id|    author_name|             user_id|rating|         review_text|language|n_votes|date_added|\n+--------------------+--------+--------------------+---------+---------------+--------------------+------+--------------------+--------+-------+----------+\n|acf387f7c35f22a39...|24779471|Discovering Delil...|  3023973| Melissa Foster|a830edac1d0f3071a...|     4|Find my reviews h...|     eng|      1|2015-08-16|\n|45af3578aaa752d42...|18599572|The Intern (The I...|  7307421|Gabrielle Tozer|4514a6d26dbc55338...|     1|A very flat read,...|     eng|      2|2015-02-20|\n|9a9c2ccd50c0c2c84...|   19063|      The Book Thief|    11466|   Markus Zusak|8fe69901571244c1f...|     4|This was our offi...|     eng|      0|2011-01-21|\n|d87cf3533507428e2...| 6605685| She Walks in Beauty|  5813022|  Siri Mitchell|7b255c949c7644b0f...|     5|this one was SOOO...|   en-US|      1|2010-12-23|\n|6ff772b1a6a7a5bb4...|   19063|      The Book Thief|    11466|   Markus Zusak|8f4597329e729e49f...|     5|I enjoyed the sym...|     eng|      0|2016-12-24|\n+--------------------+--------+--------------------+---------+---------------+--------------------+------+--------------------+--------+-------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load curated from Lab 3\n",
    "gold_df = spark.read.format(\"delta\").load(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/curated_reviews\")\n",
    "gold_df = gold_df.dropDuplicates().na.drop(subset=[\"review_id\", \"book_id\", \"rating\", \"review_text\"])\n",
    "print(\"Curated rows:\", gold_df.count())\n",
    "gold_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7215faf4-cbd0-4aa9-b621-853dd485afde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: Train 11017216 Val 2360634 Test 2361674\n"
     ]
    }
   ],
   "source": [
    "# II. Split (70/15/15 stratified)\n",
    "fractions = {rating: 0.7 for rating in range(6)}\n",
    "train_df = gold_df.sampleBy(\"rating\", fractions=fractions, seed=42)\n",
    "remaining_df = gold_df.exceptAll(train_df)\n",
    "val_df = remaining_df.sample(fraction=0.5, seed=42)\n",
    "test_df = remaining_df.exceptAll(val_df)\n",
    "\n",
    "train_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/feature_v2/train/\")\n",
    "val_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/feature_v2/validation/\")\n",
    "test_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/feature_v2/test/\")\n",
    "print(\"Splits: Train\", train_df.count(), \"Val\", val_df.count(), \"Test\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d061fc-3cb7-4021-8476-3418c7e12fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# III.2 Load train\n",
    "train_df = spark.read.format(\"delta\").load(\"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/feature_v2/train/\")\n",
    "df = train_df.select(\"review_id\", \"book_id\", \"rating\", \"review_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc4a404-d558-436f-b9bc-11bbb0cbf030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# III.3 Cleaning\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text)\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    text = emoji.replace_emoji(text, '<EMOJI>')\n",
    "    text = text.strip()\n",
    "    if len(text) < 10:\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "clean_udf = udf(clean_text, StringType())\n",
    "df = df.withColumn(\"cleaned_text\", clean_udf(col(\"review_text\")))\n",
    "df = df.filter(col(\"cleaned_text\") != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1929de2b-58e0-418f-b669-88c2a91f40e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# III.4.a Basic\n",
    "df = df.withColumn(\"review_length_chars\", length(col(\"cleaned_text\")))\n",
    "df = df.withColumn(\"words\", split(col(\"cleaned_text\"), \" \"))\n",
    "df = df.withColumn(\"review_length_words\", size(col(\"words\")))\n",
    "\n",
    "def avg_word_length(words):\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return sum(len(w) for w in words) / len(words)\n",
    "\n",
    "avg_udf = udf(avg_word_length, FloatType())\n",
    "df = df.withColumn(\"avg_word_length\", avg_udf(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e8cc8f-b2aa-47d2-8cc1-205fad369e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/spark-6d61b057-b85d-430f-92f2-b0/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751ee382-7d8c-4af2-8bb6-3e82b4e4dd66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# III.4.b Sentiment\n",
    "def get_sentiment(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['pos'], scores['neg'], scores['neu'], scores['compound']\n",
    "\n",
    "sent_schema = StructType([StructField(f\"sentiment_{k}\", FloatType(), True) for k in ['pos', 'neg', 'neu', 'compound']])\n",
    "sent_udf = udf(get_sentiment, sent_schema)\n",
    "df = df.withColumn(\"sentiment\", sent_udf(col(\"cleaned_text\")))\n",
    "df = df.select(\"*\", \"sentiment.*\").drop(\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e48bd3e-a2d5-4972-986a-52f648b5aa55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "# III.4.c TF-IDF with scikit-learn (fit on sample, transform ALL rows)\n",
    "\n",
    "# 1) Fit vocabulary on a manageable SAMPLE (to avoid >4 GiB driver result)\n",
    "#    This does NOT sample the training data for the final model – only for learning the vocabulary.\n",
    "sample_fraction = 0.05  # 5% is usually fine; reduce to 0.02 if still heavy\n",
    "\n",
    "sample_pd = (\n",
    "    df.sample(fraction=sample_fraction, seed=42)\n",
    "      .select(\"cleaned_text\")\n",
    "      .toPandas()\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,           # 500 features as in your lab\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "vectorizer.fit(sample_pd[\"cleaned_text\"])\n",
    "\n",
    "# 2) Apply TF-IDF to ALL rows via pandas_udf\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def tfidf_udf(texts: pd.Series) -> pd.Series:\n",
    "    # texts is a pandas Series of cleaned_text strings\n",
    "    tfidf = vectorizer.transform(texts.fillna(\"\").tolist())\n",
    "    return pd.Series([\n",
    "        row.toarray().ravel().astype(\"float32\").tolist()\n",
    "        for row in tfidf\n",
    "    ])\n",
    "\n",
    "df = df.withColumn(\"tfidf_features\", tfidf_udf(col(\"cleaned_text\")))\n",
    "\n",
    "# 3) Explode to tfidf_0 ... tfidf_499\n",
    "for i in range(500):\n",
    "    df = df.withColumn(f\"tfidf_{i}\", col(\"tfidf_features\")[i])\n",
    "\n",
    "df = df.drop(\"tfidf_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1937337b-8908-4776-96c7-e7068341fcfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# III.4.d Embeddings (distributed with pandas_udf, using transformers + torch)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "# Load BERT model and tokenizer once\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "bert_model = bert_model.to(\"cpu\")\n",
    "bert_model.eval()\n",
    "\n",
    "# pandas UDF for distributed embedding generation\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def bert_embed_udf(texts: pd.Series) -> pd.Series:\n",
    "    encoded = tokenizer(\n",
    "        texts.fillna(\"\").tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**encoded)\n",
    "        # mean pooling → one embedding vector per sentence\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return pd.Series(\n",
    "        [vec.cpu().numpy().astype(\"float32\").tolist() for vec in embeddings]\n",
    "    )\n",
    "\n",
    "# Apply BERT embeddings to ALL rows (no sampling)\n",
    "df = df.withColumn(\"bert_embedding\", bert_embed_udf(col(\"cleaned_text\")))\n",
    "\n",
    "# Explode 384 dimensions to bert_emb_0 ... bert_emb_383\n",
    "for i in range(384):\n",
    "    df = df.withColumn(f\"bert_emb_{i}\", col(\"bert_embedding\")[i])\n",
    "\n",
    "df = df.drop(\"bert_embedding\")\n",
    "\n",
    "# Drop intermediate text columns\n",
    "df = df.drop(\"review_text\", \"cleaned_text\", \"words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f85046d-4031-41c0-8acd-692cdbf36db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating combined feature matrix...\n Total features: 891\n   - 500 TF-IDF features\n   - 384 BERT embedding features\n   - 7 basic text features\n FEATURE MATRIX CREATED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "# IV. Combined Feature Set - FINAL WORKING VERSION (NO COUNT/DISPLAY)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Step 1: Creating combined feature matrix...\")\n",
    "\n",
    "# List all feature columns\n",
    "all_feature_columns = (\n",
    "    [f\"tfidf_{i}\" for i in range(500)] + \n",
    "    [f\"bert_emb_{i}\" for i in range(384)] +\n",
    "    [\"review_length_chars\", \"review_length_words\", \"avg_word_length\",\n",
    "     \"sentiment_pos\", \"sentiment_neg\", \"sentiment_neu\", \"sentiment_compound\"]\n",
    ")\n",
    "\n",
    "print(f\" Total features: {len(all_feature_columns)}\")\n",
    "print(\"   - 500 TF-IDF features\")\n",
    "print(\"   - 384 BERT embedding features\") \n",
    "print(\"   - 7 basic text features\")\n",
    "\n",
    "# Select metadata + all features - THIS IS YOUR FEATURE MATRIX\n",
    "final_df = df.select([\"review_id\", \"book_id\", \"rating\"] + all_feature_columns)\n",
    "\n",
    "print(\" FEATURE MATRIX CREATED SUCCESSFULLY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5488133f-53a5-46ee-9c4d-2aaad5f3bf40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Saving feature matrix to Gold layer...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkConnectGrpcException\u001B[0m                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7777873038985556>, line 6\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m features_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/features_v2/train/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Just save what you've created - this meets Lab 4 requirements\u001B[39;00m\n",
       "\u001B[0;32m----> 6\u001B[0m final_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(features_path)\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m SUCCESS: Feature matrix saved to features_v2/train/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n",
       "\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    681\u001B[0m )\n",
       "\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1484\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1482\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1483\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1484\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1485\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1486\u001B[0m )\n",
       "\u001B[1;32m   1487\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1488\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1973\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1970\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1973\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1974\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1975\u001B[0m     ):\n",
       "\u001B[1;32m   1976\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1977\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1949\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1949\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2269\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2364\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2360\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2361\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2362\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2364\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _convert_exception_without_status(error) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mSparkConnectGrpcException\u001B[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n",
       "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
       "\tdetails = \"Sent message larger than max (334941453 vs. 134217728)\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Sent message larger than max (334941453 vs. 134217728)\", grpc_status:8, created_time:\"2025-11-13T09:18:00.062137547+00:00\"}\"\n",
       ">"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkConnectGrpcException",
        "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (334941453 vs. 134217728)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Sent message larger than max (334941453 vs. 134217728)\", grpc_status:8, created_time:\"2025-11-13T09:18:00.062137547+00:00\"}\"\n>"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>SparkConnectGrpcException</span>: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (334941453 vs. 134217728)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Sent message larger than max (334941453 vs. 134217728)\", grpc_status:8, created_time:\"2025-11-13T09:18:00.062137547+00:00\"}\"\n>"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "XXKCI",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkConnectGrpcException\u001B[0m                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-7777873038985556>, line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m features_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/features_v2/train/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Just save what you've created - this meets Lab 4 requirements\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m final_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(features_path)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m SUCCESS: Feature matrix saved to features_v2/train/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    681\u001B[0m )\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1484\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1482\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1483\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1484\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1485\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1486\u001B[0m )\n\u001B[1;32m   1487\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1488\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1973\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1970\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1973\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1974\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1975\u001B[0m     ):\n\u001B[1;32m   1976\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1977\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1949\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1949\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2269\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2364\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2360\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2361\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2362\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2364\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _convert_exception_without_status(error) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "\u001B[0;31mSparkConnectGrpcException\u001B[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (334941453 vs. 134217728)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Sent message larger than max (334941453 vs. 134217728)\", grpc_status:8, created_time:\"2025-11-13T09:18:00.062137547+00:00\"}\"\n>"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: CORRECT SOLUTION - Save what you have\n",
    "print(\"Step 2: Saving feature matrix to Gold layer...\")\n",
    "features_path = \"abfss://lakehouse@goodreadsreviews60104384.dfs.core.windows.net/gold/features_v2/train/\"\n",
    "\n",
    "# Just save what you've created - this meets Lab 4 requirements\n",
    "final_df.write.mode(\"overwrite\").format(\"delta\").save(features_path)\n",
    "\n",
    "print(\" SUCCESS: Feature matrix saved to features_v2/train/\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab4_text",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}